Metric functions - example input sets (exclude ai_llm_generic_call.py)

Instructions:
- For functions that require a `log_queue`, you can use a multiprocessing.Queue() or any object exposing a `put()` method.
- For file path inputs, examples use paths relative to the repository root; replace with absolute paths if needed.

1) rampup_time_metric.rampup_time_metric(filename: str, verbosity: int, log_queue)
- Purpose: Ask LLM to score "ramp-up" time based on README.

Examples:
- Happy path (verbose logging):
  filename = "c:\\Users\\akloo\\OneDrive\\Desktop\\ECE46100\\ece30861-team-8\\README"
  verbosity = 1
  log_queue = multiprocessing.Queue()

- Silent run:
  filename = "c:\\Users\\akloo\\OneDrive\\Desktop\\ECE46100\\ece30861-team-8\\README"
  verbosity = 0
  log_queue = multiprocessing.Queue()

- Edge: non-existent file (should raise / be handled):
  filename = "c:\\path\\to\\nonexistent_readme.md"
  verbosity = 1
  log_queue = multiprocessing.Queue()

2) busFactorMetric.calculate_bus_factor(repo_owner, repo_name, verbose)
- Purpose: Calculate bus-factor-like score using GitHub pulls.

Examples:
- Typical repository:
  repo_owner = "alecandrulis"
  repo_name = "ece30861-team-8"
  verbose = 1

- Quiet mode:
  repo_owner = "tensorflow"
  repo_name = "models"
  verbose = 0

- Edge: repo with no PRs (rare) or API failure:
  repo_owner = "some_user_with_no_prs"
  repo_name = "empty-repo"
  verbose = 1

3) size metric.calculate_size_score(model_size_bytes, verbose)
- Purpose: Return device-specific size suitability scores and latency (ms).

Examples:
- Small model (50 MB):
  model_size_bytes = 50 * 1024 * 1024  # 50 MB
  verbose = 1

- Medium model (600 MB):
  model_size_bytes = 600 * 1024 * 1024  # 600 MB
  verbose = 1

- Large model (8 GB):
  model_size_bytes = 8 * 1024 * 1024 * 1024  # 8 GB
  verbose = 0

- Edge: zero bytes (invalid but test behavior):
  model_size_bytes = 0
  verbose = 1

4) code_quality.get_pylint_score(file_path: str, verbosity: int, log_queue)
- Purpose: Run pylint on a file and return scaled score and elapsed time.

Examples:
- Run on a project file (verbose):
  file_path = "c:\\Users\\akloo\\OneDrive\\Desktop\\ECE46100\\ece30861-team-8\\classes\\api.py"
  verbosity = 1
  log_queue = multiprocessing.Queue()

- Debug mode (capture full output):
  file_path = "c:\\Users\\akloo\\OneDrive\\Desktop\\ECE46100\\ece30861-team-8\\classes\\hugging_face_api.py"
  verbosity = 2
  log_queue = multiprocessing.Queue()

- Edge: pylint not installed (simulate by running in environment without pylint):
  file_path = "c:\\path\\to\\somefile.py"
  verbosity = 1
  log_queue = multiprocessing.Queue()

5) dataset_quality.evaluate_dataset_quality(dataset_name: str, verbosity: int, log_queue, split: str = "train")
- Purpose: Load HF dataset, run simple quality checks, and return score.

Examples:
- Popular dataset (IMDB):
  dataset_name = "imdb"
  split = "train"
  verbosity = 1
  log_queue = multiprocessing.Queue()

- News dataset (ag_news) with debug verbosity:
  dataset_name = "ag_news"
  split = "train"
  verbosity = 2
  log_queue = multiprocessing.Queue()

- Edge: non-existing dataset (should be handled gracefully):
  dataset_name = "nonexistent_dataset_1234"
  split = "train"
  verbosity = 1
  log_queue = multiprocessing.Queue()

6) performance_claims_metric.performance_claims_metric(filename: str, verbosity: int, log_queue)
- Purpose: Ask LLM to rate how verifiable performance claims in README are.

Examples:
- Happy path (verbose):
  filename = "c:\\Users\\akloo\\OneDrive\\Desktop\\ECE46100\\ece30861-team-8\\README"
  verbosity = 1
  log_queue = multiprocessing.Queue()

- Silent mode:
  filename = "c:\\Users\\akloo\\OneDrive\\Desktop\\ECE46100\\ece30861-team-8\\README"
  verbosity = 0
  log_queue = multiprocessing.Queue()

- Edge: README contains malformed claims or the LLM call fails:
  filename = "c:\\path\\to\\problematic_README.md"
  verbosity = 1
  log_queue = multiprocessing.Queue()

Notes on `log_queue` for testing:
- In unit tests you can replace `log_queue` with a simple stub object:
  class DummyQueue:
      def __init__(self):
          self.items = []
      def put(self, msg):
          self.items.append(msg)

  log_queue = DummyQueue()

- Or use a multiprocessing queue from the standard library:
  import multiprocessing
  log_queue = multiprocessing.Queue()

Try-it snippet (Python):

import multiprocessing
from metrics import rampup_time_metric, performance_claims_metric, code_quality, dataset_quality

q = multiprocessing.Queue()
score, elapsed = rampup_time_metric.rampup_time_metric("README", 1, q)
print(score, elapsed)

# Replace paths and dataset names as appropriate for your environment.

-- End of inputs file --
